import chromadb
from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction
import os
from transformers import AutoTokenizer,AutoConfig

# å…¨å±€å˜é‡ç¼“å­˜tokenizerå’Œconfig
_tokenizer = None
_config = None
_max_length = None

def load_tokenizer():
    global _tokenizer, _config, _max_length
    if _tokenizer is None or _config is None:
        model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
        _tokenizer = AutoTokenizer.from_pretrained(model_name)
        _config = AutoConfig.from_pretrained(model_name)
        # è·å–ä¸Šä¸‹æ–‡æœ€å¤§çš„tokené•¿åº¦
        config_dict = _config.to_dict()
        _max_length = config_dict['max_position_embeddings']
    return _tokenizer, _config, _max_length
# è·å–é›†åˆåç§° æˆ–è€…åˆ›å»ºé›†åˆ
def getCollection(collectionName:str):
  tokenizer, config, max_length = load_tokenizer()
  # Initialize embedding function
  model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
  embedding_function = SentenceTransformerEmbeddingFunction(model_name=model_name)
  # åˆ›å»ºä¸€ä¸ªæ–°çš„æ•°æ®åº“æˆ–è¿æ¥åˆ°å·²å­˜åœ¨çš„æ•°æ®åº“
  client = chromadb.Client()
  # ä½¿ç”¨æŒä¹…åŒ–å®¢æˆ·ç«¯
  # åŸä»£ç ä¸­çš„ç›¸å¯¹è·¯å¾„  
  relative_path = "./chroma_db"
  # æ–¹æ³•1ï¼šç›´æ¥ç»„åˆç»å¯¹è·¯å¾„ï¼ˆæ¨èï¼‰
  full_path = os.path.abspath(relative_path)
  print(f"å®Œæ•´å­˜å‚¨è·¯å¾„ï¼š{full_path}")
  client = chromadb.PersistentClient(path=relative_path)  # æŒ‡å®šå­˜å‚¨è·¯å¾„
  # è·å–æ‰€æœ‰é›†åˆåç§°
  existing_collections = [col for col in client.list_collections()]
  if collectionName in existing_collections:
      print("âœ… é›†åˆå·²å­˜åœ¨")
      collection = client.get_collection(
          name=collectionName,
          embedding_function=embedding_function
      )
  else:
      print("ğŸ†• é›†åˆä¸å­˜åœ¨ï¼Œæ­£åœ¨åˆ›å»º")
      collection = client.create_collection(
          name=collectionName,
          embedding_function=embedding_function
      )
  return collection

# ä½¿ç”¨å†…ç½®æˆªæ–­æœºåˆ¶
# é€šè¿‡encode_plus()çš„max_lengthå’Œtruncation=Trueå‚æ•°ï¼Œåˆ©ç”¨Hugging Faceå†…ç½®çš„æˆªæ–­ç­–ç•¥ï¼Œæ¯”æ‰‹åŠ¨åˆ‡ç‰‡æ›´å®‰å…¨å¯é 
def truncate_text(text: str) -> str:
    """æˆªæ–­æ–‡æœ¬è‡³æŒ‡å®štokenæ•°ï¼ˆåŸºäºç‰¹å®šåˆ†è¯å™¨ï¼‰"""
    tokenizer, config, max_length = load_tokenizer()
    # ç¼–ç æ—¶æ·»åŠ return_overflowing_tokenså¯æŸ¥çœ‹æº¢å‡ºéƒ¨åˆ†ï¼ˆå¯é€‰ï¼‰
    encoded = tokenizer.encode_plus(
        text,
        max_length=max_length,
        truncation=True,
        return_overflowing_tokens=False,
        return_tensors=None
    )
    # ç›´æ¥è§£ç æˆªæ–­åçš„æ–‡æœ¬
    return tokenizer.decode(encoded.input_ids, skip_special_tokens=True)

# æ–‡æ¡£æŒ‰ç…§128ä¸ªtokenåˆ†å‰²æˆå¤šä¸ªå—
def split_document_into_token_chunks(document: str) -> list:
    """
    å°†æ–‡æ¡£åˆ—è¡¨æŒ‰æŒ‡å®štokenæ•°åˆ‡åˆ†æˆå—
    :param document: æ¯ä¸ªæ–‡æ¡£æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²
    :param chunk_size: æ¯ä¸ªå—çš„tokenæ•°ï¼ˆé»˜è®¤128ï¼‰
    :return: åˆ‡åˆ†åçš„å—åˆ—è¡¨
    """
    tokenizer, config, max_length = load_tokenizer()
    # 1. å°†æ–‡æ¡£æ‹¼æ¥æˆä¸€ä¸ªé•¿å­—ç¬¦ä¸²
    full_text = document
    
    # 2. ä½¿ç”¨åˆ†è¯å™¨å°†é•¿å­—ç¬¦ä¸²ç¼–ç ä¸ºtoken
    tokens = tokenizer.encode(full_text, add_special_tokens=False)
    
    # 3. æŒ‰chunk_sizeåˆ‡åˆ†token
    chunks = [
        tokens[i:i + max_length] for i in range(0, len(tokens), max_length)
    ]
    
    # 4. å°†tokenå—è§£ç ä¸ºå­—ç¬¦ä¸²
    chunk_texts = [
        tokenizer.decode(chunk, skip_special_tokens=True) for chunk in chunks
    ]
    
    return chunk_texts
def tokenizer_test():
    text = "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•å¥å­ã€‚"
    tokens = tokenizer.tokenize(text)
    token_ids = tokenizer.encode(text)
    print("Tokens:", tokens) 
    print("Token IDs:", token_ids)
